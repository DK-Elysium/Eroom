# ======================= ì´ë ¥ì„œ OCR â†’ CSV íŒŒì´í”„ë¼ì¸ (ê°œì„  ë²„ì „) =======================
# ì‚¬ìš©ë²•: python resume_ocr_pipeline.py --input /path/to/pdf_or_folder --out resume_dataset.csv

import os
import re
import argparse
from typing import List, Dict, Any, Optional
from datetime import datetime

import fitz  # PyMuPDF
from PIL import Image
import numpy as np
import pandas as pd
import easyocr

# ---------------------------
# 1) ë¼ë²¨ ìŠ¤í‚¤ë§ˆ
# ---------------------------
RESUME_LABELS = {
    # ê¸°ë³¸ ì •ë³´
    "name": None,
    "email": None,
    "phone": None,
    
    # í•™ë ¥ ì •ë³´
    "university": None,
    "university_type": None,  # ì¸ì„œìš¸/ì§€ë°©ëŒ€/ì „ë¬¸ëŒ€/ê¸°íƒ€
    "major": None,
    "major_category": None,  # ì£¼ìš” ì „ê³µ or Other
    "gpa": None,
    "gpa_scale": None,
    "grad_year": None,
    
    # ì–´í•™ ì ìˆ˜
    "english_test_type": None,  # TOEIC/TOEFL/IELTS
    "english_score": None,
    
    # ì¸í„´ ê²½í—˜
    "intern_experiences": [],  # [{company, company_scale, months}]
    "intern_count": 0,
    "intern_total_months": 0,
    
    # ìˆ˜ìƒ ê²½ë ¥
    "awards": [],  # [{name, scale}]
    "award_count": 0,
    
    # í”„ë¡œì íŠ¸
    "projects": [],
    "project_count": 0,
    
    # ìê²©ì¦
    "certifications": [],
    "certification_count": 0,
    
    # í•´ì™¸ ê²½í—˜
    "overseas_experiences": [],  # [{type, country, duration}]
    "overseas_count": 0,
    
    # ë©”íƒ€ ì •ë³´
    "source_pdf": None,
    "error": None,
}

# ---------------------------
# 2) ì°¸ì¡° ë°ì´í„°
# ---------------------------
# ì„œìš¸ ì†Œì¬ ëŒ€í•™êµ (ì£¼ìš” ëŒ€í•™)
SEOUL_UNIVERSITIES = [
    "ì„œìš¸ëŒ€", "ì—°ì„¸ëŒ€", "ê³ ë ¤ëŒ€", "ì„±ê· ê´€ëŒ€", "í•œì–‘ëŒ€", "ì¤‘ì•™ëŒ€", "ê²½í¬ëŒ€", "í•œêµ­ì™¸ëŒ€",
    "ì„œìš¸ì‹œë¦½ëŒ€", "ê±´êµ­ëŒ€", "ë™êµ­ëŒ€", "í™ìµëŒ€", "ìˆ™ëª…ì—¬ëŒ€", "ì´í™”ì—¬ëŒ€", "ì„œê°•ëŒ€",
    "Seoul National", "Yonsei", "Korea University", "Sungkyunkwan", "Hanyang",
    "Chung-Ang", "Kyung Hee", "HUFS", "Seoul National University of Science"
]

# ì „ë¬¸ëŒ€ í‚¤ì›Œë“œ
COLLEGE_KEYWORDS = ["ì „ë¬¸ëŒ€", "College", "Polytechnic", "ê¸°ëŠ¥ëŒ€í•™"]

# ì£¼ìš” ì „ê³µ ë¦¬ìŠ¤íŠ¸
MAJOR_CATEGORIES = {
    "ì»´í“¨í„°ê³µí•™": ["ì»´í“¨í„°", "Computer Science", "Software", "ì†Œí”„íŠ¸ì›¨ì–´"],
    "ì „ê¸°ì „ìê³µí•™": ["ì „ê¸°", "ì „ì", "Electrical", "Electronic", "ë°˜ë„ì²´"],
    "ê¸°ê³„ê³µí•™": ["ê¸°ê³„", "Mechanical", "ìë™ì°¨"],
    "í™”í•™ê³µí•™": ["í™”í•™", "Chemical", "í™”ê³µ"],
    "ê²½ì˜í•™": ["ê²½ì˜", "Business", "Management", "MBA"],
    "ê²½ì œí•™": ["ê²½ì œ", "Economics"],
    "ì‚°ì—…ê³µí•™": ["ì‚°ì—…ê³µí•™", "Industrial Engineering"],
    "ê±´ì¶•í•™": ["ê±´ì¶•", "Architecture"],
    "ìƒëª…ê³µí•™": ["ìƒëª…", "Biotechnology", "Bio"],
    "ìˆ˜í•™": ["ìˆ˜í•™", "Mathematics", "í†µê³„"],
    "ë¬¼ë¦¬í•™": ["ë¬¼ë¦¬", "Physics"],
    "ë””ìì¸": ["ë””ìì¸", "Design"],
}

# ëŒ€ê¸°ì—… ë¦¬ìŠ¤íŠ¸ (í™•ì¥ ê°€ëŠ¥)
LARGE_COMPANIES = [
    "ì‚¼ì„±", "Samsung", "LG", "SK", "í˜„ëŒ€", "Hyundai", "ê¸°ì•„", "Kia",
    "í¬ìŠ¤ì½”", "POSCO", "í•œí™”", "Hanwha", "ë¡¯ë°", "Lotte", "GS", "ë‘ì‚°", "Doosan",
    "ë„¤ì´ë²„", "Naver", "ì¹´ì¹´ì˜¤", "Kakao", "ì¿ íŒ¡", "Coupang", "ë°°ë‹¬ì˜ë¯¼ì¡±", 
    "êµ¬ê¸€", "Google", "ë©”íƒ€", "Meta", "ì•„ë§ˆì¡´", "Amazon", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "Microsoft",
    "ì• í”Œ", "Apple", "í…ŒìŠ¬ë¼", "Tesla", "ë„·í”Œë¦­ìŠ¤", "Netflix"
]

# ì¤‘ê²¬ê¸°ì—… í‚¤ì›Œë“œ
MIDSIZE_KEYWORDS = ["ì¤‘ê²¬", "ì½”ìŠ¤ë‹¥", "KOSDAQ", "ìƒì¥"]

# ---------------------------
# 3) PDF â†’ ì´ë¯¸ì§€
# ---------------------------
def pdf_to_images(pdf_path: str, dpi: int = 200) -> List[Image.Image]:
    """PDFë¥¼ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    images: List[Image.Image] = []
    doc = fitz.open(pdf_path)
    for page in doc:
        pix = page.get_pixmap(dpi=dpi, alpha=False)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    doc.close()
    return images

# ---------------------------
# 4) OCR ìˆ˜í–‰
# ---------------------------
def ocr_images(images: List[Image.Image], languages: List[str]) -> str:
    """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    reader = easyocr.Reader(languages, gpu=False)
    all_texts: List[str] = []
    
    for img in images:
        texts = reader.readtext(np.array(img), detail=0)
        page_text = "\n".join(t.strip() for t in texts if t.strip())
        all_texts.append(page_text)
    
    return "\n".join(all_texts)

# ---------------------------
# 5) ëŒ€í•™êµ ë¶„ë¥˜
# ---------------------------
def classify_university(university_name: str) -> str:
    """ëŒ€í•™ì„ ì¸ì„œìš¸/ì§€ë°©ëŒ€/ì „ë¬¸ëŒ€/ê¸°íƒ€ë¡œ ë¶„ë¥˜"""
    if not university_name:
        return "ê¸°íƒ€"
    
    # ì „ë¬¸ëŒ€ ì²´í¬
    for keyword in COLLEGE_KEYWORDS:
        if keyword in university_name:
            return "ì „ë¬¸ëŒ€"
    
    # ì¸ì„œìš¸ ì²´í¬
    for seoul_uni in SEOUL_UNIVERSITIES:
        if seoul_uni in university_name:
            return "ì¸ì„œìš¸"
    
    # ëŒ€í•™êµ/ëŒ€í•™ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì§€ë°©ëŒ€
    if "ëŒ€í•™êµ" in university_name or "University" in university_name:
        return "ì§€ë°©ëŒ€"
    
    return "ê¸°íƒ€"

# ---------------------------
# 6) ì „ê³µ ë¶„ë¥˜
# ---------------------------
def classify_major(major_name: str) -> str:
    """ì „ê³µì„ ì£¼ìš” ì¹´í…Œê³ ë¦¬ ë˜ëŠ” Otherë¡œ ë¶„ë¥˜"""
    if not major_name:
        return "Other"
    
    for category, keywords in MAJOR_CATEGORIES.items():
        for keyword in keywords:
            if keyword.lower() in major_name.lower():
                return category
    
    return "Other"

# ---------------------------
# 7) íšŒì‚¬ ê·œëª¨ ë¶„ë¥˜
# ---------------------------
def classify_company_scale(company_name: str) -> str:
    """íšŒì‚¬ë¥¼ ëŒ€ê¸°ì—…/ì¤‘ê²¬/ì¤‘ì†Œë¡œ ë¶„ë¥˜"""
    if not company_name:
        return "ì¤‘ì†Œ"
    
    # ëŒ€ê¸°ì—… ì²´í¬
    for large_comp in LARGE_COMPANIES:
        if large_comp.lower() in company_name.lower():
            return "ëŒ€ê¸°ì—…"
    
    # ì¤‘ê²¬ê¸°ì—… ì²´í¬
    for keyword in MIDSIZE_KEYWORDS:
        if keyword in company_name:
            return "ì¤‘ê²¬"
    
    return "ì¤‘ì†Œ"

# ---------------------------
# 8) ìˆ˜ìƒ ê·œëª¨ ë¶„ë¥˜
# ---------------------------
def classify_award_scale(award_text: str) -> str:
    """ìˆ˜ìƒì„ êµ­ì œ/ì „êµ­/ì§€ì—­/êµë‚´ë¡œ ë¶„ë¥˜"""
    award_lower = award_text.lower()
    
    if any(kw in award_lower for kw in ["international", "world", "global", "êµ­ì œ", "ì„¸ê³„"]):
        return "êµ­ì œ"
    elif any(kw in award_lower for kw in ["national", "ì „êµ­", "í•œêµ­", "korea"]):
        return "ì „êµ­"
    elif any(kw in award_lower for kw in ["regional", "ì§€ì—­", "ì‹œ", "ë„"]):
        return "ì§€ì—­"
    elif any(kw in award_lower for kw in ["university", "college", "school", "ëŒ€í•™", "êµë‚´", "í•™êµ"]):
        return "êµë‚´"
    
    return "êµë‚´"  # ê¸°ë³¸ê°’

# ---------------------------
# 9) ì •ê·œì‹ í—¬í¼
# ---------------------------
def _match_first(regexes: List[re.Pattern], text: str) -> Optional[str]:
    """ì—¬ëŸ¬ ì •ê·œì‹ ì¤‘ ì²« ë²ˆì§¸ ë§¤ì¹­ ê²°ê³¼ ë°˜í™˜"""
    for rgx in regexes:
        m = rgx.search(text)
        if m:
            return m.group(1) if m.groups() else m.group(0)
    return None

# ---------------------------
# 10) í…ìŠ¤íŠ¸ íŒŒì‹± (í•µì‹¬ ë¡œì§)
# ---------------------------
def parse_text_to_features(text: str) -> Dict[str, Any]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì´ë ¥ì„œ ì •ë³´ ì¶”ì¶œ"""
    data = {k: (v.copy() if isinstance(v, (list, dict)) else v) for k, v in RESUME_LABELS.items()}
    
    norm = text.replace("\r", "\n")
    lines = [ln.strip() for ln in norm.split("\n") if ln.strip()]
    lower = norm.lower()

    # ========== ê¸°ë³¸ ì •ë³´ ==========
    # ì´ë¦„ (ìµœìƒë‹¨ ë¼ì¸)
    if lines:
        top = lines[0]
        if len(top) <= 50 and not re.search(r"@|github|linkedin|http", top, re.I):
            data["name"] = top

    # ì´ë©”ì¼
    email = _match_first([re.compile(r"([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,})")], norm)
    data["email"] = email

    # ì „í™”ë²ˆí˜¸
    phone = _match_first([
        re.compile(r"(\+82[-\s]?\d{1,2}[-\s]?\d{3,4}[-\s]?\d{4})"),
        re.compile(r"(01[0-9][-\s]?\d{3,4}[-\s]?\d{4})"),
    ], norm)
    data["phone"] = phone

    # ========== í•™ë ¥ ì •ë³´ ==========
    # ëŒ€í•™êµ
    uni = _match_first([
        re.compile(r"([A-Z][A-Za-z&.\s]{2,}University)"),
        re.compile(r"([ê°€-í£A-Za-z&.\s]{2,}ëŒ€í•™êµ)"),
        re.compile(r"([ê°€-í£A-Za-z&.\s]{2,}ëŒ€í•™)"),
    ], norm)
    data["university"] = uni
    if uni:
        data["university_type"] = classify_university(uni)

    # ì „ê³µ
    major = _match_first([
        re.compile(r"(Computer Science|Software Engineering|Electrical Engineering|Information Technology|Data Science)", re.I),
        re.compile(r"([ê°€-í£A-Za-z/&\s]{2,20}(?:ê³¼|í•™ê³¼|ì „ê³µ))"),
    ], norm)
    data["major"] = major
    if major:
        data["major_category"] = classify_major(major)

    # í•™ì 
    gpa_match = _match_first([
        re.compile(r"GPA\s*[:=]?\s*([0-4]\.\d{1,2})\s*/\s*([0-4]\.?\d*)", re.I),
        re.compile(r"í‰ì \s*[:=]?\s*([0-4]\.\d{1,2})\s*/\s*([0-4]\.?\d*)"),
    ], norm)
    if gpa_match:
        parts = re.findall(r"(\d\.\d+)", gpa_match)
        if len(parts) >= 2:
            data["gpa"] = float(parts[0])
            data["gpa_scale"] = float(parts[1])
        elif len(parts) == 1:
            data["gpa"] = float(parts[0])
            data["gpa_scale"] = 4.5

    # ì¡¸ì—… ì—°ë„
    grad_year = _match_first([
        re.compile(r"(20\d{2})\s*(?:ë…„)?\s*(?:ì¡¸ì—…|Graduation)", re.I)
    ], norm)
    if grad_year:
        data["grad_year"] = int(grad_year)

    # ========== ì–´í•™ ì ìˆ˜ ==========
    toeic = _match_first([re.compile(r"TOEIC\s*[:=]?\s*(\d{3,4})", re.I)], norm)
    ielts = _match_first([re.compile(r"IELTS\s*[:=]?\s*(\d(?:\.\d)?)", re.I)], norm)
    toefl = _match_first([re.compile(r"TOEFL\s*[:=]?\s*(\d{2,3})", re.I)], norm)
    
    if toeic:
        data["english_test_type"] = "TOEIC"
        data["english_score"] = int(toeic)
    elif ielts:
        data["english_test_type"] = "IELTS"
        data["english_score"] = float(ielts)
    elif toefl:
        data["english_test_type"] = "TOEFL"
        data["english_score"] = int(toefl)

    # ========== ì¸í„´ ê²½í—˜ ==========
    # ì¸í„´ í‚¤ì›Œë“œ ì°¾ê¸°
    intern_patterns = [
        re.compile(r"(ì¸í„´|Intern|Internship)\s+(?:at\s+)?([^\n,]+?)(?:\s+\()?(\d+)\s*(?:ê°œì›”|months?|mos?)", re.I),
        re.compile(r"([^\n,]+?)\s+(?:ì¸í„´|Intern).*?(\d+)\s*(?:ê°œì›”|months?)", re.I),
    ]
    
    intern_list = []
    for pattern in intern_patterns:
        for match in pattern.finditer(norm):
            try:
                if len(match.groups()) >= 3:
                    company = match.group(2).strip()
                    months = int(match.group(3))
                elif len(match.groups()) >= 2:
                    company = match.group(1).strip()
                    months = int(match.group(2))
                else:
                    continue
                
                scale = classify_company_scale(company)
                intern_list.append({
                    "company": company,
                    "company_scale": scale,
                    "months": months
                })
            except:
                continue
    
    data["intern_experiences"] = intern_list
    data["intern_count"] = len(intern_list)
    data["intern_total_months"] = sum(i["months"] for i in intern_list)

    # ========== ìˆ˜ìƒ ê²½ë ¥ ==========
    award_patterns = [
        re.compile(r"(ìˆ˜ìƒ|Award|Prize)\s*[:ï¼š]?\s*([^\n]+)", re.I),
        re.compile(r"([^\n]+?)\s+(?:ëŒ€íšŒ|Competition|Contest).*?(?:ìˆ˜ìƒ|ìƒ|Award|Prize)", re.I),
    ]
    
    award_list = []
    for pattern in award_patterns:
        for match in pattern.finditer(norm):
            award_text = match.group(2) if len(match.groups()) >= 2 else match.group(1)
            award_text = award_text.strip()[:100]  # ê¸¸ì´ ì œí•œ
            
            scale = classify_award_scale(award_text)
            award_list.append({
                "name": award_text,
                "scale": scale
            })
    
    # ì¤‘ë³µ ì œê±°
    seen = set()
    unique_awards = []
    for award in award_list:
        key = award["name"][:50]
        if key not in seen:
            seen.add(key)
            unique_awards.append(award)
    
    data["awards"] = unique_awards
    data["award_count"] = len(unique_awards)

    # ========== í”„ë¡œì íŠ¸ ==========
    project_patterns = [
        re.compile(r"(Project|í”„ë¡œì íŠ¸)\s*[:ï¼š]?\s*([^\n]+)", re.I),
    ]
    
    project_list = []
    for pattern in project_patterns:
        for match in pattern.finditer(norm):
            proj_name = match.group(2).strip()[:100]
            if proj_name and len(proj_name) > 3:
                project_list.append(proj_name)
    
    data["projects"] = list(set(project_list))[:10]  # ìµœëŒ€ 10ê°œ
    data["project_count"] = len(data["projects"])

    # ========== ìê²©ì¦ ==========
    cert_patterns = [
        re.compile(r"(ìê²©ì¦|Certificate|Certification)\s*[:ï¼š]?\s*([^\n]+)", re.I),
        re.compile(r"([ê°€-í£A-Za-z\s]+ê¸°ì‚¬)", re.I),
        re.compile(r"(SQLD|ADsP|ì •ë³´ì²˜ë¦¬ê¸°ì‚¬|ë„¤íŠ¸ì›Œí¬ê´€ë¦¬ì‚¬)", re.I),
    ]
    
    cert_list = []
    for pattern in cert_patterns:
        for match in pattern.finditer(norm):
            cert_name = match.group(2) if len(match.groups()) >= 2 else match.group(1)
            cert_name = cert_name.strip()[:50]
            if cert_name and len(cert_name) > 2:
                cert_list.append(cert_name)
    
    data["certifications"] = list(set(cert_list))[:10]
    data["certification_count"] = len(data["certifications"])

    # ========== í•´ì™¸ ê²½í—˜ ==========
    overseas_patterns = [
        re.compile(r"(êµí™˜í•™ìƒ|Exchange Student|Study Abroad).*?([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)", re.I),
        re.compile(r"(ì–´í•™ì—°ìˆ˜|Language Training).*?([A-Z][a-z]+)", re.I),
        re.compile(r"(í•´ì™¸ ì¸í„´|International Intern).*?([A-Z][a-z]+)", re.I),
    ]
    
    overseas_list = []
    for pattern in overseas_patterns:
        for match in pattern.finditer(norm):
            exp_type = match.group(1).strip()
            country = match.group(2).strip() if len(match.groups()) >= 2 else "Unknown"
            
            overseas_list.append({
                "type": exp_type,
                "country": country,
                "duration": None  # ê¸°ê°„ì€ ì¶”ê°€ íŒŒì‹± í•„ìš”
            })
    
    data["overseas_experiences"] = overseas_list
    data["overseas_count"] = len(overseas_list)

    return data

# ---------------------------
# 11) ë‹¨ì¼ PDF ì²˜ë¦¬
# ---------------------------
def process_resume(pdf_path: str, languages: List[str], dpi: int = 200) -> Dict[str, Any]:
    """PDF ì´ë ¥ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ"""
    try:
        # 1. PDF â†’ ì´ë¯¸ì§€
        images = pdf_to_images(pdf_path, dpi=dpi)
        
        # 2. OCR ìˆ˜í–‰
        ocr_text = ocr_images(images, languages=languages)
        
        # 3. PyMuPDF í…ìŠ¤íŠ¸ ë ˆì´ì–´ë„ ì¶”ì¶œ (ë³´ê°•)
        doc = fitz.open(pdf_path)
        pdf_text = []
        for page in doc:
            pdf_text.append(page.get_text("text"))
        doc.close()
        
        # ë‘ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ê¸´ ê²ƒ ìš°ì„ )
        combined_text = ocr_text if len(ocr_text) > len("\n".join(pdf_text)) else "\n".join(pdf_text)
        
        # 4. íŒŒì‹±
        features = parse_text_to_features(combined_text)
        features["source_pdf"] = os.path.basename(pdf_path)
        
        return features
        
    except Exception as e:
        row = {k: None for k in RESUME_LABELS.keys()}
        row["source_pdf"] = os.path.basename(pdf_path)
        row["error"] = str(e)
        return row

# ---------------------------
# 12) ë°°ì¹˜ ì²˜ë¦¬
# ---------------------------
def batch_build_dataset(input_path: str, languages: List[str], dpi: int = 200) -> pd.DataFrame:
    """ì—¬ëŸ¬ PDFë¥¼ ì²˜ë¦¬í•˜ì—¬ ë°ì´í„°ì…‹ ìƒì„±"""
    paths: List[str] = []
    
    if os.path.isdir(input_path):
        for name in os.listdir(input_path):
            if name.lower().endswith(".pdf"):
                paths.append(os.path.join(input_path, name))
    elif os.path.isfile(input_path) and input_path.lower().endswith(".pdf"):
        paths = [input_path]
    else:
        raise FileNotFoundError("PDF íŒŒì¼ ë˜ëŠ” PDFê°€ ìˆëŠ” í´ë”ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

    print(f"ğŸ“„ ì´ {len(paths)}ê°œì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    
    rows = []
    for idx, pdf_path in enumerate(sorted(paths), 1):
        print(f"[{idx}/{len(paths)}] ì²˜ë¦¬ ì¤‘: {os.path.basename(pdf_path)}")
        result = process_resume(pdf_path, languages=languages, dpi=dpi)
        rows.append(result)
    
    return pd.DataFrame(rows)

# ---------------------------
# 13) CLI
# ---------------------------
def main():
    parser = argparse.ArgumentParser(
        description="ì´ë ¥ì„œ PDFë¥¼ OCRë¡œ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ CSV ë°ì´í„°ì…‹ ìƒì„±"
    )
    parser.add_argument("--input", required=True, help="PDF íŒŒì¼ ê²½ë¡œ ë˜ëŠ” í´ë” ê²½ë¡œ")
    parser.add_argument("--out", default="resume_dataset.csv", help="ì €ì¥í•  CSV íŒŒì¼ëª… (ê¸°ë³¸: resume_dataset.csv)")
    parser.add_argument("--dpi", type=int, default=200, help="PDF ë Œë”ë§ DPI (ê¸°ë³¸: 200)")
    parser.add_argument("--langs", nargs="+", default=["ko", "en"], help="EasyOCR ì–¸ì–´ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ko en)")
    
    args = parser.parse_args()

    # ë°ì´í„°ì…‹ ìƒì„±
    df = batch_build_dataset(args.input, languages=args.langs, dpi=args.dpi)
    
    # CSV ì €ì¥
    df.to_csv(args.out, index=False, encoding="utf-8-sig")
    
    print(f"\nâœ… ì™„ë£Œ! {len(df)}ê°œì˜ ì´ë ¥ì„œ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“Š ì €ì¥ ê²½ë¡œ: {os.path.abspath(args.out)}")
    print(f"\nğŸ“ˆ í†µê³„:")
    print(f"  - ì¸ì„œìš¸: {(df['university_type'] == 'ì¸ì„œìš¸').sum()}ëª…")
    print(f"  - ì§€ë°©ëŒ€: {(df['university_type'] == 'ì§€ë°©ëŒ€').sum()}ëª…")
    print(f"  - ì „ë¬¸ëŒ€: {(df['university_type'] == 'ì „ë¬¸ëŒ€').sum()}ëª…")
    print(f"  - í‰ê·  ì¸í„´ ê²½í—˜: {df['intern_count'].mean():.1f}íšŒ")
    print(f"  - í‰ê·  ìˆ˜ìƒ: {df['award_count'].mean():.1f}íšŒ")

if __name__ == "__main__":
    main()